{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "indian-volume",
   "metadata": {},
   "source": [
    "## Importando dependÃªncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aggregate-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "brief-identification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.metrics import Precision\n",
    "from tensorflow.keras.metrics import Recall\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "from tensorflow.keras.losses import Loss\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-copper",
   "metadata": {},
   "source": [
    "## Importando dataset e splitando em teste e treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "thrown-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_time_windows_dataset(dataset, window_size = 20, step_size = 1, is_label = False):\n",
    "    # split com janelas rolante\n",
    "    splitted_dataset = list()\n",
    "    split_size = len(dataset) - (window_size - step_size)\n",
    "    offset = 0\n",
    "    \n",
    "    for _ in range(split_size):\n",
    "        if is_label:\n",
    "            splitted_dataset.append(dataset[offset:(window_size + offset)])\n",
    "        else:\n",
    "            splitted_dataset.append(dataset[offset:(window_size + offset), :])\n",
    "        offset += step_size\n",
    "        \n",
    "    final_dataset = np.array(splitted_dataset)                            \n",
    "    return final_dataset\n",
    "\n",
    "def balance_dataset_labels(dataset):\n",
    "    balanced_dataset = dataset.loc[dataset.iloc[:, -1].isin([251, 12, 3, 24, 7, 8, 6])]\n",
    "    standart_label_dataset = dataset.loc[dataset.iloc[:, -1] == 0]\n",
    "    slice_dataset = standart_label_dataset.iloc[0:2200]\n",
    "    balanced_dataset = balanced_dataset.append(slice_dataset)\n",
    "    balanced_dataset = shuffle(balanced_dataset)\n",
    "    balanced_dataset.reset_index(inplace=True, drop=True)\n",
    "    return balanced_dataset\n",
    "\n",
    "\n",
    "def normalize_dataset_labels(dataset):\n",
    "    y = dataset.iloc[:, -1].values\n",
    "    unique_labels = sorted(list(set(y)))\n",
    "    for i in range(len(y)):\n",
    "        y[i] = unique_labels.index(y[i])\n",
    "\n",
    "    dataset.iloc[:, -1] = y\n",
    "\n",
    "HEADER = [\"timestamp\",'srcip', 'srcport', 'dstip', 'dstport', 'proto', 'total_fpackets', 'total_fvolume',\n",
    "          'total_bpackets', 'total_bvolume', 'min_fpktl', 'mean_fpktl', 'max_fpktl', 'std_fpktl', 'min_bpktl',\n",
    "          'mean_bpktl', 'max_bpktl', 'std_bpktl', 'min_fiat', 'mean_fiat', 'max_fiat', 'std_fiat', 'min_biat',\n",
    "          'mean_biat', 'max_biat', 'std_biat', 'duration', 'min_active', 'mean_active', 'max_active', 'std_active',\n",
    "          'min_idle', 'mean_idle', 'max_idle', 'std_idle', 'sflow_fpackets', 'sflow_fbytes', 'sflow_bpackets',\n",
    "          'sflow_bbytes', 'fpsh_cnt', 'bpsh_cnt', 'furg_cnt', 'burg_cnt', 'total_fhlen', 'total_bhlen', \"dscp\", 'class']\n",
    "initial_dataset = pd.read_csv('dataset-oi-2017-02-24.csv', names=HEADER)\n",
    "\n",
    "# initial_dataset = balance_dataset_labels(initial_dataset)\n",
    "\n",
    "normalize_dataset_labels(initial_dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-bracket",
   "metadata": {},
   "source": [
    "## Criando modelo da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "severe-copper",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    def build(self, shape = (41,), classes = 141):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(200, input_shape=shape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(200))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))            \n",
    "        \n",
    "        self.built = model\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def build_conv(self, shape = (None, 20, 41), classes = 141):\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(filters=32, kernel_size=3, input_shape=shape))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "        model.add(Conv1D(filters=32, kernel_size=3))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(32))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "            \n",
    "        self.built = model\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def compile(self, learning_rate = 0.01, comms_round = 100):\n",
    "        self.built.compile(loss = 'sparse_categorical_crossentropy', \n",
    "                          optimizer = SGD(learning_rate=learning_rate, decay=learning_rate / comms_round, momentum=0.9),\n",
    "                        metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-musical",
   "metadata": {},
   "source": [
    "## Criando modelo do servidor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "democratic-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server:\n",
    "    def __init__(self, dataset = list(), model = Net(), learning_rate = 0.01, comms_round = 100, num_clients = 5):\n",
    "        self.num_clients = num_clients\n",
    "        self.dataset = dataset\n",
    "        X = self.dataset.iloc[:, 5:-1].values\n",
    "        y = self.dataset.iloc[:, -1].values\n",
    "        X = np.array(X, dtype=np.float64)\n",
    "        y = np.array(y, dtype=np.float64)\n",
    "        \n",
    "        self.num_labels = len(set(y))\n",
    "        \n",
    "#         X = split_time_windows_dataset(X)\n",
    "#         y = split_time_windows_dataset(y, is_label = True)\n",
    "        \n",
    "        self.input_shape = X.shape[1:]\n",
    "        \n",
    "        self.global_model = model\n",
    "#         self.global_model.build_conv(classes = self.num_labels, shape = self.input_shape)\n",
    "        self.global_model.build(classes = self.num_labels, shape = self.input_shape)\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.comms_round = comms_round\n",
    "        self.global_model.compile(self.learning_rate, self.comms_round)\n",
    "        \n",
    "    def split_dataset(self, clients):\n",
    "        num_clients = len(clients)\n",
    "        split_size = len(self.dataset) // num_clients\n",
    "        splitted_dataset = [self.dataset[i:i + split_size] for i in range(0, split_size * num_clients, split_size)]\n",
    "        for i in range(0, num_clients):\n",
    "            local_dataset = splitted_dataset[i]\n",
    "            clients[i].local_dataset = local_dataset\n",
    "            clients[i].num_labels = self.num_labels\n",
    "            clients[i].input_shape = self.input_shape\n",
    "            \n",
    "    def split_dataset_non_iid(self, clients):\n",
    "        unique_ips = self.dataset['srcip'].value_counts().head(self.num_clients).keys()\n",
    "        for i in range(0, self.num_clients):\n",
    "            local_dataset = self.dataset.loc[self.dataset['srcip'] == unique_ips[i]]\n",
    "            clients[i].local_dataset = local_dataset\n",
    "            clients[i].num_labels = self.num_labels\n",
    "            clients[i].input_shape = self.input_shape\n",
    "            \n",
    "        \n",
    "    def agregate_models(self, clients_train_data):\n",
    "        _, clients_data_size = zip(*clients_train_data)\n",
    "        global_data_size = sum(list(clients_data_size))\n",
    "        scaled_models_weights = list()\n",
    "        \n",
    "        for local_weights, local_data_size in clients_train_data:\n",
    "            client_mean = local_data_size / global_data_size\n",
    "            scaled_local_weights = list()\n",
    "            \n",
    "            # Scaling each model weight for each client model with dataset size mean\n",
    "            for i in range(len(local_weights)):\n",
    "                scaled_weight = local_weights[i] * client_mean\n",
    "                scaled_local_weights.append(scaled_weight)\n",
    "                \n",
    "            scaled_models_weights.append(scaled_local_weights)\n",
    "           \n",
    "        average_model_weights = list()\n",
    "        # agragate local models weights to global model\n",
    "        for model_weights in zip(*scaled_models_weights):\n",
    "            layer_mean = tf.math.reduce_sum(model_weights, axis=0)\n",
    "            average_model_weights.append(layer_mean)\n",
    "            \n",
    "        self.global_model.built.set_weights(average_model_weights)\n",
    "        \n",
    "    def send_global_model_weights(self, clients):\n",
    "        global_model_weights = self.global_model.built.get_weights()\n",
    "        for client in clients:\n",
    "            client.set_model_weights(global_model_weights)\n",
    "            client.num_labels = self.num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-expert",
   "metadata": {},
   "source": [
    "## Criando modelo do cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "compatible-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, client_id = 123, dataset = [1.0, 2.0], model = Net(), num_labels = 141):\n",
    "        self.client_id = client_id\n",
    "        self.local_dataset = pd.DataFrame(dataset)\n",
    "        self.local_model = model\n",
    "        self.num_labels = num_labels\n",
    "        self.metrics_history = list()\n",
    "        \n",
    "    def preprocess(self):\n",
    "        X = self.local_dataset.iloc[:, 5:-1].values\n",
    "        y = self.local_dataset.iloc[:, -1].values\n",
    "        \n",
    "        X = np.array(X, dtype=np.float64)\n",
    "        y = np.array(y, dtype=np.float64)\n",
    "        \n",
    "        sc = StandardScaler()\n",
    "        X = sc.fit_transform(X)\n",
    "        \n",
    "        oversample = RandomOverSampler()\n",
    "        X, y = oversample.fit_resample(X, y)\n",
    "        \n",
    "#         X = split_time_windows_dataset(X)\n",
    "#         y = split_time_windows_dataset(y, is_label = True)\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "        \n",
    "    def train_model(self):\n",
    "        self.batch_size = 128\n",
    "        self.local_model.compile()\n",
    "        self.local_model.built.fit(self.X_train, \n",
    "                                   self.y_train, \n",
    "                                   batch_size = self.batch_size, \n",
    "                                   epochs=1, \n",
    "                                   verbose=0)\n",
    "        print()\n",
    "        \n",
    "    def test_model(self):\n",
    "        print(f\"[INFO] Local test of client {self.client_id}\")\n",
    "        y_pred = self.local_model.built.predict(self.X_test).argmax(axis=1)\n",
    "        \n",
    "        precision = Precision()\n",
    "        precision.update_state(self.y_test, y_pred)\n",
    "        prec = precision.result().numpy()\n",
    "        \n",
    "        recall = Recall()\n",
    "        recall.update_state(self.y_test, y_pred)\n",
    "        rec = recall.result().numpy()\n",
    "        \n",
    "        losses = Loss()\n",
    "        loss = losses.call(self.y_test, y_pred)\n",
    "        \n",
    "        accuracy = Accuracy()\n",
    "        accuracy.update_state(self.y_test, y_pred)\n",
    "        acc = accuracy.result().numpy()\n",
    "        \n",
    "        self.metrics_history.append((loss, acc, prec, rec))\n",
    "        \n",
    "        cm = confusion_matrix(self.y_test, y_pred)\n",
    "        print(cm) \n",
    "        \n",
    "    def plot_result(self, rounds):\n",
    "        loss_history, acc_history, prec_history, rec_history = zip(*self.metrics_history)\n",
    "        epochs = range(1, rounds + 1)\n",
    "        plt.plot(epochs, loss_history, 'g', label='Loss')\n",
    "        plt.plot(epochs, acc_history, 'b', label='Accuracy')\n",
    "        plt.title(f'Loss and Accuracy history of client {self.client_id}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss and Accuracy')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        plt.plot(epochs, prec_history, 'r', label='Precision')\n",
    "        plt.plot(epochs, rec_history, 'k', label='Recall')\n",
    "        plt.title(f'Precision and Recall history of client {self.client_id}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Precision and Recall')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    def set_model_weights(self, global_model_weights):\n",
    "#         self.local_model.build_conv(classes = self.num_labels, shape = self.input_shape)\n",
    "        self.local_model.build(classes = self.num_labels, shape = self.input_shape)\n",
    "        self.local_model.built.set_weights(global_model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-atmosphere",
   "metadata": {},
   "source": [
    "## Splitando dataset para os clientes da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eleven-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients = 10\n",
    "learning_rate = 0.01\n",
    "train_rounds = 5\n",
    "server = Server(initial_dataset, learning_rate = learning_rate, comms_round = train_rounds, num_clients = num_clients)\n",
    "clients = [Client(client_id = i) for i in range(num_clients)]\n",
    "\n",
    "server.split_dataset_non_iid(clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-garage",
   "metadata": {},
   "source": [
    "## Treinando e testando modelos NN locais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ordinary-corruption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Local test of client 0\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "predictions must be <= 1\nCondition x <= y did not hold.\nFirst 3 elements of x:\n[ 9. 58. 24.]\nFirst 1 elements of y:\n[1.]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-72882d4eaaaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlocal_clients_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_dataset_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mevaluation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[INFO] Local train {i} finished!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-fbaf972313a6>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPrecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mprecision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mprec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-n/lib/python3.9/site-packages/tensorflow/python/keras/utils/metrics_utils.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_context_for_symbolic_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m       \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_state_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mupdate_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# update_op will be None in eager execution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m       \u001b[0mmetric_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-n/lib/python3.9/site-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36mupdate_state_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mcontrol_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mag_update_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_update_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mag_update_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdef_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-n/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-n/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-n/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-n/lib/python3.9/site-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36mupdate_state\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mUpdate\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \"\"\"\n\u001b[0;32m-> 1327\u001b[0;31m     return metrics_utils.update_confusion_matrix_variables(\n\u001b[0m\u001b[1;32m   1328\u001b[0m         {\n\u001b[1;32m   1329\u001b[0m             \u001b[0mmetrics_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfusionMatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRUE_POSITIVES\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrue_positives\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-n/lib/python3.9/site-packages/tensorflow/python/keras/utils/metrics_utils.py\u001b[0m in \u001b[0;36mupdate_confusion_matrix_variables\u001b[0;34m(variables_to_update, y_true, y_pred, thresholds, top_k, class_id, sample_weight, multi_label, label_weights)\u001b[0m\n\u001b[1;32m    351\u001b[0m           \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m           message='predictions must be >= 0'),\n\u001b[0;32m--> 353\u001b[0;31m       check_ops.assert_less_equal(\n\u001b[0m\u001b[1;32m    354\u001b[0m           \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m           \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-n/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-n/lib/python3.9/site-packages/tensorflow/python/ops/check_ops.py\u001b[0m in \u001b[0;36massert_less_equal\u001b[0;34m(x, y, data, summarize, message, name)\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_binary_assert_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0massert_less_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m   return _binary_assert('<=', 'assert_less_equal', math_ops.less_equal,\n\u001b[0m\u001b[1;32m    947\u001b[0m                         np.less_equal, x, y, data, summarize, message, name)\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-n/lib/python3.9/site-packages/tensorflow/python/ops/check_ops.py\u001b[0m in \u001b[0;36m_binary_assert\u001b[0;34m(sym, opname, op_func, static_func, x, y, data, summarize, message, name)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m       raise errors.InvalidArgumentError(\n\u001b[0m\u001b[1;32m    354\u001b[0m           \u001b[0mnode_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: predictions must be <= 1\nCondition x <= y did not hold.\nFirst 3 elements of x:\n[ 9. 58. 24.]\nFirst 1 elements of y:\n[1.]"
     ]
    }
   ],
   "source": [
    "for i in range(train_rounds):\n",
    "    local_clients_train = list()\n",
    "    # atualiza os pesos dos modelos locais antes do treinamento\n",
    "    server.send_global_model_weights(clients)\n",
    "\n",
    "    for client in clients:\n",
    "        client.preprocess()\n",
    "        client.train_model()\n",
    "\n",
    "        # envia pesos do modelo local e tamanho do dataset local de cada cliente pro servidor\n",
    "        local_weight = client.local_model.built.get_weights()\n",
    "        local_dataset_size = len(client.local_dataset)\n",
    "        local_clients_train.append((local_weight, local_dataset_size))\n",
    "\n",
    "        client.test_model()\n",
    "        \n",
    "    print(f\"[INFO] Local train {i} finished!\")\n",
    "    server.agregate_models(local_clients_train)\n",
    "    \n",
    "print(f\"[INFO] All local trains are finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-description",
   "metadata": {},
   "source": [
    "## Plotando resultados dos treinos locais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_metrics = [[],[],[],[]]\n",
    "for client in clients:\n",
    "    client.plot_result(train_rounds)\n",
    "    for i in range(4):\n",
    "        final_metrics[i].append(client.metrics_history[-1][i])\n",
    "       \n",
    "plt.boxplot(final_metrics)\n",
    "plt.title('Final metrics result')\n",
    "plt.xticks([1,2,3,4], ['loss', 'accuracy', 'precision', 'recall'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
